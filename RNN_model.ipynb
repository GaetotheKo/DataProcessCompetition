{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7010ab",
   "metadata": {},
   "source": [
    "# DataCenter Storage Fail Prediction using RNN(LSTM) model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30345167",
   "metadata": {},
   "source": [
    "### Import all packages used at below codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41d0e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler \n",
    "\n",
    "from numpy import array \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM \n",
    "from keras.layers import Dropout, BatchNormalization, Activation \n",
    "\n",
    "import tensorflow as tf \n",
    "from keras.models import load_model \n",
    "\n",
    "\n",
    "from csv import writer \n",
    "from csv import reader \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b249b07",
   "metadata": {},
   "source": [
    "### Input & output file name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2c7543ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_train = 'train_sample.csv' \n",
    "input_file_testA = 'testA_sample.csv' \n",
    "input_file_testB = 'testB_sample.csv' \n",
    "input_file_problem_A = 'testA_problem.csv' \n",
    "input_file_problem_B = 'testB_problem.csv' \n",
    "\n",
    "from datetime import datetime\n",
    "postfix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "output_file_problem_A = 'output_A_' + postfix + '.csv' \n",
    "output_file_problem_B = 'output_B_' + postfix + '.csv' \n",
    "\n",
    "checkpoint_filename = 'my_model_checkpoint_' + postfix + '.h5' \n",
    "my_model_filename = 'my_model_' + postfix +'.h5' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a403e7",
   "metadata": {},
   "source": [
    "### Read input csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9a89baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    " \n",
    "\n",
    "use_feature_Cols = [0,1,2,3,4,5,6,7,8,10,11,12,13,14,19,20] #choose use columns except 0, 1, 2, 20 \n",
    "#exclude columns: 9, 15, 16, 17, 18 \n",
    "\n",
    "df_train = pd.read_csv(input_file_train, usecols=use_feature_Cols) \n",
    "df_testA = pd.read_csv(input_file_testA, usecols=use_feature_Cols[0:-1]) \n",
    "df_testB = pd.read_csv(input_file_testB, usecols=use_feature_Cols[0:-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eeee30",
   "metadata": {},
   "source": [
    "### Transform df to array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bedb556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.array(df_train.iloc[:,0:-1]) \n",
    "X_testA_all = np.array(df_testA.iloc[:,0:]) \n",
    "X_testB_all = np.array(df_testB.iloc[:,0:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56136dd1",
   "metadata": {},
   "source": [
    "### Aggregation all data for scaling(normalizing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5129d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_data_train = X_train_all.shape[0] \n",
    "num_of_data_testA = X_testA_all.shape[0] \n",
    "num_of_data_testB = X_testB_all.shape[0] \n",
    "num_of_data_all = num_of_data_train + num_of_data_testA + num_of_data_testB \n",
    "\n",
    "exclude = [1, 2] \n",
    "num_of_features = X_train_all.shape[1] - len(exclude) \n",
    "\n",
    "X_train_exclude = np.zeros((num_of_data_train, num_of_features)) \n",
    "X_testA_exclude = np.zeros((num_of_data_testA, num_of_features)) \n",
    "X_testB_exclude = np.zeros((num_of_data_testB, num_of_features)) \n",
    "X_all_exclude = np.zeros((num_of_data_all,num_of_features)) \n",
    "\n",
    "i = 0 \n",
    "for j in range(len(X_train_all)): \n",
    "    X_train_exclude[j] = np.delete(X_train_all[j], exclude) \n",
    "    X_all_exclude[i] = np.delete(X_train_all[j], exclude) \n",
    "    i += 1 \n",
    "\n",
    "for j in range(len(X_testA_all)): \n",
    "    X_testA_exclude[j] = np.delete(X_testA_all[j], exclude) \n",
    "    X_all_exclude[i] = np.delete(X_testA_all[j], exclude) \n",
    "    i += 1 \n",
    "\n",
    "for j in range(len(X_testB_all)): \n",
    "    X_testB_exclude[j] = np.delete(X_testB_all[j], exclude) \n",
    "    X_all_exclude[i] = np.delete(X_testA_all[j], exclude) \n",
    "    i += 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d8337",
   "metadata": {},
   "source": [
    "### Scaling library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2a10f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler \n",
    "\n",
    "mm = MinMaxScaler() \n",
    "ss = StandardScaler() \n",
    "rs = RobustScaler(quantile_range=(10.0,90.0)) \n",
    "\n",
    "#choose standard scaler: transform data variation to -1 < data < 1 \n",
    "#scaler = mm \n",
    "scaler = ss \n",
    "#scaler = rs \n",
    "\n",
    "X_all_exclude_scaled = scaler.fit_transform(X_all_exclude) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6de6e4",
   "metadata": {},
   "source": [
    "### Scaling all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a6aac656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_exclude_scaled = scaler.transform(X_train_exclude) \n",
    "X_testA_exclude_scaled = scaler.transform(X_testA_exclude) \n",
    "X_testB_exclude_scaled = scaler.transform(X_testB_exclude) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef714c08",
   "metadata": {},
   "source": [
    "### Preprocess data transform 2d array to 3d array (for input of LSTM model) – 1/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88a63857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_serial_number: 2\n",
      "max_num_of_data_per_samples: 5\n"
     ]
    }
   ],
   "source": [
    "num_of_serial_number = 0 \n",
    "max_num_of_data_per_samples = 0 \n",
    "\n",
    "def CountNumDataAndSamples(X_original): \n",
    "    global num_of_serial_number \n",
    "    global max_num_of_data_per_samples \n",
    "    num_of_data_per_samples = 0 \n",
    "    for i in range(len(X_original)): \n",
    "        num_of_data_per_samples += 1 \n",
    "        if i == len(X_original) - 1: \n",
    "            num_of_serial_number += 1 \n",
    "        else: \n",
    "            if not X_original[i][0] == X_original[i+1][0]: \n",
    "                num_of_serial_number += 1 \n",
    "                if max_num_of_data_per_samples < num_of_data_per_samples: \n",
    "                    max_num_of_data_per_samples = num_of_data_per_samples \n",
    "                num_of_data_samples = 0 \n",
    "\n",
    "CountNumDataAndSamples(X_train_exclude_scaled) \n",
    "print(\"num_of_serial_number:\", num_of_serial_number)\n",
    "print(\"max_num_of_data_per_samples:\", max_num_of_data_per_samples) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d3dd1",
   "metadata": {},
   "source": [
    "### Preprocess data transform 2d array to 3d array – 2/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f81d1443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_serial_number: 2\n",
      "max_num_of_data_per_samples: 5\n",
      "num_use_features: 13\n",
      "num_total_data: 10\n",
      "num_class_zero: 1\n",
      "num_class_one: 1\n"
     ]
    }
   ],
   "source": [
    "X_original = X_train_exclude_scaled \n",
    "\n",
    "df_serial_number_only = df_train.iloc[:,0] \n",
    "df_label = df_train.iloc[:,-1] \n",
    "y_original = np.array(df_label) \n",
    "\n",
    "i = num_of_serial_number \n",
    "j = max_num_of_data_per_samples \n",
    "num_use_features = X_original.shape[1] \n",
    "k = num_use_features \n",
    "\n",
    "num_total_data = X_original.shape[0] \n",
    "print(\"num_of_serial_number:\", num_of_serial_number) \n",
    "print(\"max_num_of_data_per_samples:\",max_num_of_data_per_samples) \n",
    "print(\"num_use_features:\",num_use_features) \n",
    "print(\"num_total_data:\",num_total_data) \n",
    "\n",
    "X = np.zeros((i, j, k)) \n",
    "y = np.zeros((i,1)) \n",
    "\n",
    "serial_number = df_serial_number_only[0] \n",
    "serial_number_idx = 0 \n",
    "per_data_idx = 0 \n",
    "num_class_zero = 0 \n",
    "num_class_one = 0 \n",
    "\n",
    "for idx in range(num_total_data): \n",
    "    if not df_serial_number_only[idx] == serial_number: \n",
    "        serial_number = df_serial_number_only[idx] \n",
    "        y[serial_number_idx][0] = y_original[idx-1] \n",
    "        if y_original[idx-1] == 0: \n",
    "            num_class_zero += 1 \n",
    "        else: \n",
    "            num_class_one += 1 \n",
    "        serial_number_idx += 1 \n",
    "        per_data_idx = 0 \n",
    "    X[serial_number_idx][per_data_idx] = X_original[idx] \n",
    "    per_data_idx += 1 \n",
    "\n",
    "#last Y \n",
    "idx += 1 \n",
    "y[serial_number_idx][0] = y_original[idx-1] \n",
    "if y_original[idx-1] == 0: \n",
    "    num_class_zero += 1 \n",
    "else: \n",
    "    num_class_one += 1 \n",
    "\n",
    "print(\"num_class_zero:\", num_class_zero) \n",
    "print(\"num_class_one:\", num_class_one) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884da58",
   "metadata": {},
   "source": [
    "### Oversampling for label 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60e8651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oversampling done\n"
     ]
    }
   ],
   "source": [
    "i = num_class_zero * 2 \n",
    "\n",
    "X_resample = np.zeros((i, j, k)) \n",
    "y_resample = np.zeros((i,1)) \n",
    "\n",
    "new_idx = 0 \n",
    "for idx in range(num_of_serial_number): \n",
    "    if y[idx] == 0: \n",
    "        X_resample[new_idx] = X[idx] \n",
    "        y_resample[new_idx] = y[idx] \n",
    "        new_idx += 1 \n",
    "\n",
    "new_idx = num_class_zero \n",
    "\n",
    "end = 1 \n",
    "while end: \n",
    "    for idx in range(num_of_serial_number): \n",
    "        if y[idx] == 1: \n",
    "            X_resample[new_idx] = X[idx] \n",
    "            y_resample[new_idx] = y[idx] \n",
    "            new_idx += 1 \n",
    "\n",
    "        if new_idx >= (num_class_zero * 2 - 1): \n",
    "            end = 0 \n",
    "            break \n",
    "\n",
    "print(\"oversampling done\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429e32a",
   "metadata": {},
   "source": [
    "### Make LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "868a51e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 5, 16)             1920      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 5, 16)             2112      \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,762\n",
      "Trainable params: 9,570\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy import array \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM \n",
    "from keras.layers import Dropout, BatchNormalization, Activation \n",
    "\n",
    "model = Sequential() \n",
    "num_of_output_LSTM = 16 \n",
    "\n",
    "model.add(LSTM(num_of_output_LSTM, return_sequences=True, input_shape=(max_num_of_data_per_samples, num_use_features))) \n",
    "model.add(LSTM(num_of_output_LSTM, return_sequences=True)) \n",
    "model.add(LSTM(num_of_output_LSTM)) \n",
    "model.add(Dropout(0.1)) \n",
    "\n",
    "model.add(Dense(64)) \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Activation('relu')) \n",
    "\n",
    "model.add(Dense(32)) \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Activation('relu')) \n",
    "\n",
    "model.add(Dense(2)) \n",
    "model.add(Activation('softmax')) \n",
    "\n",
    "model.summary() \n",
    "\n",
    "model.compile(loss='SparseCategoricalCrossentropy', optimizer='adam', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac139d",
   "metadata": {},
   "source": [
    "### Define checkpoint callback function to save temperal result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6408b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4b3a9",
   "metadata": {},
   "source": [
    "### Define decaying learning rate callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6969921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "def scheduler(epoch, lr): \n",
    "    print(\"learning rate:\", lr) \n",
    "    return (0.01) * ((1/2) ** (epoch / 10)) \n",
    "\n",
    "callback_decay_lr = tf.keras.callbacks.LearningRateScheduler(scheduler) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b805415",
   "metadata": {},
   "source": [
    "### define early stopping callback function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b88da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "epoch = 40 \n",
    "\n",
    "callback_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=epoch/10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b851ae3",
   "metadata": {},
   "source": [
    "### model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f6cc6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "learning rate: 0.0010000000474974513\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.6931 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68326, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 2/40\n",
      "learning rate: 0.009999999776482582\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6832 - accuracy: 1.0000 - val_loss: 0.6741 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68326 to 0.67414, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 3/40\n",
      "learning rate: 0.009330329485237598\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6740 - accuracy: 1.0000 - val_loss: 0.6657 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67414 to 0.66569, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 4/40\n",
      "learning rate: 0.008705505169928074\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6655 - accuracy: 1.0000 - val_loss: 0.6580 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66569 to 0.65796, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 5/40\n",
      "learning rate: 0.008122524246573448\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6577 - accuracy: 1.0000 - val_loss: 0.6508 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65796 to 0.65079, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 6/40\n",
      "learning rate: 0.007578582968562841\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6504 - accuracy: 1.0000 - val_loss: 0.6441 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.65079 to 0.64413, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 7/40\n",
      "learning rate: 0.007071067579090595\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6437 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.64413 to 0.63802, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 8/40\n",
      "learning rate: 0.006597539409995079\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6375 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.63802 to 0.63236, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 9/40\n",
      "learning rate: 0.006155721843242645\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6317 - accuracy: 1.0000 - val_loss: 0.6272 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.63236 to 0.62715, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 10/40\n",
      "learning rate: 0.005743491929024458\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6264 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.62715 to 0.62225, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 11/40\n",
      "learning rate: 0.005358867347240448\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6214 - accuracy: 1.0000 - val_loss: 0.6178 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.62225 to 0.61776, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 12/40\n",
      "learning rate: 0.004999999888241291\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6169 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.61776 to 0.61361, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 13/40\n",
      "learning rate: 0.004665164742618799\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6126 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.61361 to 0.60977, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 14/40\n",
      "learning rate: 0.004352752584964037\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6087 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.60977 to 0.60623, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 15/40\n",
      "learning rate: 0.004061262123286724\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6050 - accuracy: 1.0000 - val_loss: 0.6029 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.60623 to 0.60294, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 16/40\n",
      "learning rate: 0.0037892914842814207\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6016 - accuracy: 1.0000 - val_loss: 0.5999 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.60294 to 0.59988, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 17/40\n",
      "learning rate: 0.0035355337895452976\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5985 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.59988 to 0.59699, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 18/40\n",
      "learning rate: 0.0032987697049975395\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5956 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.59699 to 0.59439, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 19/40\n",
      "learning rate: 0.0030778609216213226\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5929 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.59439 to 0.59185, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 20/40\n",
      "learning rate: 0.002871745964512229\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5903 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.59185 to 0.58960, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 21/40\n",
      "learning rate: 0.002679433673620224\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5880 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.58960 to 0.58755, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 22/40\n",
      "learning rate: 0.0024999999441206455\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5858 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.58755 to 0.58557, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 23/40\n",
      "learning rate: 0.0023325823713093996\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5838 - accuracy: 1.0000 - val_loss: 0.5838 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.58557 to 0.58379, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 24/40\n",
      "learning rate: 0.0021763762924820185\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5819 - accuracy: 1.0000 - val_loss: 0.5821 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.58379 to 0.58208, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 25/40\n",
      "learning rate: 0.002030631061643362\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5802 - accuracy: 1.0000 - val_loss: 0.5806 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.58208 to 0.58057, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 26/40\n",
      "learning rate: 0.0018946457421407104\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5785 - accuracy: 1.0000 - val_loss: 0.5792 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.58057 to 0.57917, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 27/40\n",
      "learning rate: 0.0017677668947726488\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5770 - accuracy: 1.0000 - val_loss: 0.5779 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.57917 to 0.57789, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 28/40\n",
      "learning rate: 0.0016493848524987698\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5756 - accuracy: 1.0000 - val_loss: 0.5766 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.57789 to 0.57663, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 29/40\n",
      "learning rate: 0.0015389304608106613\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5743 - accuracy: 1.0000 - val_loss: 0.5755 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_loss improved from 0.57663 to 0.57554, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 30/40\n",
      "learning rate: 0.0014358729822561145\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5731 - accuracy: 1.0000 - val_loss: 0.5745 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.57554 to 0.57453, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 31/40\n",
      "learning rate: 0.001339716836810112\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5720 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.57453 to 0.57355, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 32/40\n",
      "learning rate: 0.0012499999720603228\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5709 - accuracy: 1.0000 - val_loss: 0.5727 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.57355 to 0.57270, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 33/40\n",
      "learning rate: 0.0011662911856546998\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5699 - accuracy: 1.0000 - val_loss: 0.5719 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.57270 to 0.57193, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 34/40\n",
      "learning rate: 0.0010881881462410092\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5690 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.57193 to 0.57120, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 35/40\n",
      "learning rate: 0.001015315530821681\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5681 - accuracy: 1.0000 - val_loss: 0.5705 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.57120 to 0.57054, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 36/40\n",
      "learning rate: 0.0009473228710703552\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5674 - accuracy: 1.0000 - val_loss: 0.5699 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.57054 to 0.56991, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 37/40\n",
      "learning rate: 0.0008838834473863244\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5666 - accuracy: 1.0000 - val_loss: 0.5693 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.56991 to 0.56935, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 38/40\n",
      "learning rate: 0.0008246924262493849\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5659 - accuracy: 1.0000 - val_loss: 0.5688 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.56935 to 0.56882, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 39/40\n",
      "learning rate: 0.0007694652304053307\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5653 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.56882 to 0.56835, saving model to my_model_checkpoint_20210522_134615.h5\n",
      "Epoch 40/40\n",
      "learning rate: 0.0007179364911280572\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5647 - accuracy: 1.0000 - val_loss: 0.5679 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.56835 to 0.56793, saving model to my_model_checkpoint_20210522_134615.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dc05871670>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_resample, y_resample, epochs=epoch, batch_size=128, shuffle=True, validation_split=0.1, callbacks=[callback_decay_lr, callback_early_stop, callback_checkpoint]) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8cf0b",
   "metadata": {},
   "source": [
    "### load model from checkpoint file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddca8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "model_last = model \n",
    "\n",
    "model = load_model(checkpoint_filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88403b0b",
   "metadata": {},
   "source": [
    "### save model for next time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "be8e89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "model.save(my_model_filename) \n",
    "\n",
    "reconstructed_model = load_model(my_model_filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ba491",
   "metadata": {},
   "source": [
    "### prediction for testA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21836a0",
   "metadata": {},
   "source": [
    "### data preprocessing for prediction testA 1/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "98a5a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_serial_number: 2\n",
      "max_num_of_data_per_samples: 5\n"
     ]
    }
   ],
   "source": [
    "num_of_serial_number = 0 \n",
    "\n",
    "CountNumDataAndSamples(X_testA_exclude_scaled) \n",
    "print(\"num_of_serial_number:\", num_of_serial_number) \n",
    "print(\"max_num_of_data_per_samples:\", max_num_of_data_per_samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61314fc9",
   "metadata": {},
   "source": [
    "### Data preprocessing for prediction testA – 2/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1790b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_serial_number: 2\n",
      "max_num_of_data_per_samples: 5\n",
      "num_use_features: 13\n",
      "num_total_data: 10\n"
     ]
    }
   ],
   "source": [
    "X_original = X_testA_exclude_scaled \n",
    "\n",
    "df = df_testA \n",
    "df_serial_number_only = df.iloc[:, 0] \n",
    "\n",
    "df_label = df.iloc[:, -1] \n",
    "y_original = np.array(df_label) \n",
    "\n",
    "i = num_of_serial_number \n",
    "j = max_num_of_data_per_samples \n",
    "num_use_features = X_original.shape[1] \n",
    "k = num_use_features \n",
    "\n",
    "num_total_data = X_original.shape[0] \n",
    "\n",
    "print(\"num_of_serial_number:\", num_of_serial_number)\n",
    "print(\"max_num_of_data_per_samples:\", max_num_of_data_per_samples)\n",
    "print(\"num_use_features:\", num_use_features)\n",
    "print(\"num_total_data:\", num_total_data)\n",
    " \n",
    "X = np.zeros((i, j, k)) \n",
    "y = np.zeros((i, 1)) \n",
    "\n",
    "serial_number = df_serial_number_only[0] \n",
    "serial_number_idx = 0 \n",
    "per_data_idx = 0 \n",
    "\n",
    "for idx in range(num_total_data): \n",
    "    if not df_serial_number_only[idx] == serial_number: \n",
    "        serial_number = df_serial_number_only[idx] \n",
    "        y[serial_number_idx][0] = y_original[idx-1]\n",
    "        serial_number_idx += 1 \n",
    "        per_data_idx = 0 \n",
    "    X[serial_number_idx][per_data_idx] = X_original[idx] \n",
    "    per_data_idx += 1 \n",
    "\n",
    "#last Y \n",
    "idx += 1 \n",
    "y[serial_number_idx][0] = y_original[idx-1] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e4ebd",
   "metadata": {},
   "source": [
    "### Prediction for testA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ad6257d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [4 0]\n",
      " [5 2]\n",
      " [6 0]\n",
      " [7 0]\n",
      " [8 0]\n",
      " [9 0]]\n",
      "(2, 1)\n",
      "my_result: 2 / 0\n",
      "\n",
      "0.9792: 2494 / 461\n",
      "0.9857: 2490 / 465\n"
     ]
    }
   ],
   "source": [
    "X_testA_for_validation = X \n",
    "\n",
    "yhat = model.predict(X_testA_for_validation) \n",
    "print(yhat.shape) \n",
    "\n",
    "num_zero = 0 \n",
    "num_one = 0 \n",
    "y_result = np.zeros((num_of_serial_number, 1), dtype=int) \n",
    "for i in range(len(yhat)): \n",
    "    if yhat[i][0] >= yhat[i][1]: \n",
    "        y_result[i][0] = 0\n",
    "        num_zero += 1 \n",
    "    else: \n",
    "        y_result[i][0] = 1 \n",
    "        num_one += 1 \n",
    "\n",
    "y_result_variation = np.zeros((10,2),dtype=int) \n",
    "\n",
    "for j in range(len(y_result_variation)): \n",
    "    y_result_variation[j][0] = j \n",
    "\n",
    "for i in range(len(yhat)): \n",
    "    for j in range(len(y_result_variation)): \n",
    "        if yhat[i][0] >= 0.1 * y_result_variation[j][0]: \n",
    "            if j == 9: \n",
    "                y_result_variation[j][1] += 1 \n",
    "                continue \n",
    "            if yhat[i][0] < 0.1 * y_result_variation[j+1][0]: \n",
    "                y_result_variation[j][1] += 1 \n",
    "\n",
    "print(y_result_variation) \n",
    "\n",
    "print(y_result.shape) \n",
    "print(\"my_result:\", num_zero, \"/\", num_one) \n",
    "print(\"\\n0.9792: 2494 / 461\") \n",
    "print(\"0.9857: 2490 / 465\") \n",
    "\n",
    "y_result_A = y_result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4a34f",
   "metadata": {},
   "source": [
    "### Define output file save function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4b4e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer \n",
    "from csv import reader \n",
    "\n",
    "def add_column_in_csv(input_file, output_file, y_result): \n",
    "    with open(input_file, 'r')  as read_obj, \\\n",
    "        open(output_file, 'w', newline='') as write_obj: \n",
    "            csv_reader = reader(read_obj) \n",
    "            csv_writer = writer(write_obj) \n",
    "            init_line = 0 \n",
    "            count = 0 \n",
    "            for row in csv_reader: \n",
    "                if init_line == 0: \n",
    "                    csv_writer.writerow(row) \n",
    "                    init_line = 1 \n",
    "                    continue \n",
    "                else:\n",
    "                    del row[-1]\n",
    "                    row.append(y_result[count][0]) \n",
    "                    csv_writer.writerow(row) \n",
    "                    count += 1 \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8da017",
   "metadata": {},
   "source": [
    "### Save output file include test A prediction result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "16d1b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_column_in_csv(input_file_problem_A, output_file_problem_A, y_result_A) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "Same for testB \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
